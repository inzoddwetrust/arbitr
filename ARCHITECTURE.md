# KAD Parser: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –ü–ª–∞–Ω –†–∞–∑–≤–∏—Ç–∏—è

> –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –ø–∞—Ä—Å–µ—Ä–∞ kad.arbitr.ru –≤ LLM-–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫—É—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è —é—Ä–∏—Å—Ç–æ–≤.

---

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞](#1-–æ–±–∑–æ—Ä-–ø—Ä–æ–µ–∫—Ç–∞)
2. [–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (Phase 0)](#2-—Ç–µ–∫—É—â–µ–µ-—Å–æ—Å—Ç–æ—è–Ω–∏–µ-phase-0)
3. [–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã](#3-–≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ-–ø—Ä–æ–±–ª–µ–º—ã)
4. [LLM-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–ª—è —é—Ä–∏—Å—Ç–æ–≤](#4-llm-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞-–¥–ª—è-—é—Ä–∏—Å—Ç–æ–≤)
5. [RAG-—Å–∏—Å—Ç–µ–º–∞](#5-rag-—Å–∏—Å—Ç–µ–º–∞)
6. [–¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#6-—Ü–µ–ª–µ–≤–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
7. [–ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏](#7-–ø–ª–∞–Ω-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)

---

## 1. –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞

### 1.1 –¶–µ–ª—å

–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö –¥–µ–ª —Å –ø–æ–º–æ—â—å—é LLM:
- **–ü–∞—Ä—Å–∏–Ω–≥** –¥–µ–ª —Å kad.arbitr.ru (–æ–±—Ö–æ–¥ WASM-–∑–∞—â–∏—Ç—ã)
- **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ** —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—É–¥–µ–±–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- **–ê–Ω–∞–ª–∏–∑** —Å–∏–ª—å–Ω—ã—Ö/—Å–ª–∞–±—ã—Ö –º–µ—Å—Ç —Å—Ç–æ—Ä–æ–Ω
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏** —é—Ä–∏—Å—Ç–∞–º –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

### 1.2 –ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏

| –†–æ–ª—å | –ó–∞–¥–∞—á–∞ | –ö–∞–∫ –ø–æ–º–æ–≥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞ |
|------|--------|---------------------|
| –Æ—Ä–∏—Å—Ç –∏—Å—Ç—Ü–∞ | –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ–∑–∏—Ü–∏–∏ | –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç –æ—Ç–≤–µ—Ç—á–∏–∫–∞, –ø–æ–¥–±–æ—Ä –ø—Ä–∞–∫—Ç–∏–∫–∏ |
| –Æ—Ä–∏—Å—Ç –æ—Ç–≤–µ—Ç—á–∏–∫–∞ | –ó–∞—â–∏—Ç–∞ | –ü–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π, –∫–æ–Ω—Ç—Ä–∞—Ä–≥—É–º–µ–Ω—Ç—ã |
| –ê–Ω–∞–ª–∏—Ç–∏–∫ | Due diligence | –ë—ã—Å—Ç—Ä—ã–π –æ–±–∑–æ—Ä —Å—É–¥–µ–±–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ |
| –°—É–¥—å—è/–ø–æ–º–æ—â–Ω–∏–∫ | –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞—Å–µ–¥–∞–Ω–∏—é | –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è –¥–µ–ª–∞ |

---

## 2. –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (Phase 0)

### 2.1 –ß—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

```
arbitr/
‚îú‚îÄ‚îÄ poc2.py          # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–µ—Ä (1248 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ poc.py           # –†–∞–Ω–Ω—è—è –≤–µ—Ä—Å–∏—è (761 —Å—Ç—Ä–æ–∫–∞)
‚îú‚îÄ‚îÄ pdf2json.py      # –£—Ç–∏–ª–∏—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (85 —Å—Ç—Ä–æ–∫)
‚îú‚îÄ‚îÄ ROADMAP.md       # –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è
‚îú‚îÄ‚îÄ PHASE0_REPORT.md # –û—Ç—á—ë—Ç –æ Phase 0
‚îî‚îÄ‚îÄ CODE_REVIEW.md   # –†–µ–≤—å—é –∫–æ–¥–∞
```

### 2.2 –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–µ—Ä–∞

| –§—É–Ω–∫—Ü–∏—è | –°—Ç–∞—Ç—É—Å | –û–ø–∏—Å–∞–Ω–∏–µ |
|---------|--------|----------|
| –û–±—Ö–æ–¥ WASM-–∑–∞—â–∏—Ç—ã | ‚úÖ | Firefox + response interceptor |
| –ü–æ–∏—Å–∫ –ø–æ –Ω–æ–º–µ—Ä—É –¥–µ–ª–∞ | ‚úÖ | –ß–µ—Ä–µ–∑ suggest dropdown |
| –ü–∞—Ä—Å–∏–Ω–≥ "–°—É–¥–µ–±–Ω—ã–µ –∞–∫—Ç—ã" | ‚úÖ | –ë–µ–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ |
| –ü–∞—Ä—Å–∏–Ω–≥ "–ö–∞—Ä—Ç–æ—á–∫–∏" | ‚úÖ | –° –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–∞–º–∏ –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π |
| –ü–∞—Ä—Å–∏–Ω–≥ "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ" | ‚úÖ | –° –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π |
| –°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF | ‚úÖ | –ß–µ—Ä–µ–∑ response interceptor |
| –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ | ‚úÖ | PyMuPDF |
| Human-like –∑–∞–¥–µ—Ä–∂–∫–∏ | ‚úÖ | –†–∞–Ω–¥–æ–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—É–∑—ã |
| Graceful stop | ‚úÖ | –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—Ä–∏ rate limit |
| –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è | ‚úÖ | –ü–æ doc_id |

### 2.3 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã–≤–æ–¥–∞

```
case_A60-21280-2023/
‚îú‚îÄ‚îÄ README.md              # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚îú‚îÄ‚îÄ case.json              # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–µ–ª–∞
‚îú‚îÄ‚îÄ court_acts.json        # –°–ø–∏—Å–æ–∫ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤
‚îú‚îÄ‚îÄ electronic_case.json   # –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –≠–î
‚îú‚îÄ‚îÄ instances/
‚îÇ   ‚îî‚îÄ‚îÄ inst_{guid}.json   # –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º
‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îî‚îÄ‚îÄ {guid}.json        # –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îî‚îÄ‚îÄ _progress.json         # –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
```

---

## 3. –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

### 3.1 –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ (P0)

#### 3.1.1 –ü—Ä–æ–±–ª–µ–º–∞ —Å refresh —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Ä—ã–≤–∞

**–°–∏–º–ø—Ç–æ–º:** –ü–æ—Å–ª–µ `take_break()` –ø–µ—Ä–≤—ã–µ 3-5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è.

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ refresh –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ WASM.

```python
# –¢–µ–∫—É—â–∏–π –∫–æ–¥ (poc2.py:1118-1123)
await page.goto(case_info.url, wait_until="domcontentloaded", timeout=30000)
await page.wait_for_timeout(2000)  # ‚Üê –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ!
```

**–†–µ—à–µ–Ω–∏–µ:**
```python
async def refresh_session_with_warmup(page: Page, case_url: str) -> None:
    """Refresh —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–æ–≥—Ä–µ–≤–æ–º WASM."""

    # 1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–µ–ª–∞
    await page.goto(case_url, wait_until="domcontentloaded", timeout=30000)
    await page.wait_for_timeout(3000)

    # 2. –ü—Ä–æ–≥—Ä–µ—Ç—å WASM —á–µ—Ä–µ–∑ –æ—Ç–∫—Ä—ã—Ç–∏–µ –ª—é–±–æ–≥–æ PDF
    pdf_link = page.locator("a[href*='PdfDocument']").first
    if await pdf_link.count() > 0:
        url = await pdf_link.get_attribute("href")
        if url:
            warmup_page = await page.context.new_page()
            try:
                await warmup_page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await warmup_page.wait_for_timeout(5000)  # –î–æ–∂–¥–∞—Ç—å—Å—è WASM
            except Exception as e:
                log.debug(f"Warmup failed (expected): {e}")
            finally:
                await warmup_page.close()

    # 3. –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –≤–∫–ª–∞–¥–∫–µ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    acts_tab = page.locator("#case_acts")
    if await acts_tab.count() > 0:
        await acts_tab.click()
        await page.wait_for_timeout(2000)
```

#### 3.1.2 –ü—É—Å—Ç—ã–µ except –±–ª–æ–∫–∏

**–ü—Ä–æ–±–ª–µ–º–∞:** –ì–ª—É—à–∞—Ç—Å—è –≤—Å–µ –æ—à–∏–±–∫–∏, —É—Å–ª–æ–∂–Ω—è—è –æ—Ç–ª–∞–¥–∫—É.

```python
# –ü–ª–æ—Ö–æ (poc2.py:186-188)
except:
    pass

# –•–æ—Ä–æ—à–æ
except PlaywrightError as e:
    log.debug(f"Rate limit check failed: {e}")
```

#### 3.1.3 –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ graceful shutdown

**–ü—Ä–æ–±–ª–µ–º–∞:** –ü—Ä–∏ Ctrl+C –±—Ä–∞—É–∑–µ—Ä –º–æ–∂–µ—Ç –æ—Å—Ç–∞—Ç—å—Å—è –æ—Ç–∫—Ä—ã—Ç—ã–º.

**–†–µ—à–µ–Ω–∏–µ:**
```python
import signal
import sys

class GracefulShutdown:
    def __init__(self):
        self.shutdown_requested = False
        signal.signal(signal.SIGINT, self._handler)
        signal.signal(signal.SIGTERM, self._handler)

    def _handler(self, signum, frame):
        log.info(f"–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –∑–∞–≤–µ—Ä—à–∞–µ–º...")
        self.shutdown_requested = True

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
shutdown = GracefulShutdown()

for doc in docs_to_download:
    if shutdown.shutdown_requested:
        save_progress(output_dir, progress)
        break
    # ... download logic
```

### 3.2 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ (P1)

#### 3.2.1 –ú–æ–Ω–æ–ª–∏—Ç–Ω—ã–π —Ñ–∞–π–ª

**–ü—Ä–æ–±–ª–µ–º–∞:** 1248 —Å—Ç—Ä–æ–∫ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ, —Å–º–µ—à–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π.

**–†–µ—à–µ–Ω–∏–µ:** –ú–æ–¥—É–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å–º. —Ä–∞–∑–¥–µ–ª 6).

#### 3.2.2 –•–∞—Ä–¥–∫–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
BASE_URL = "https://kad.arbitr.ru/"
HEADLESS = True
DELAY_BETWEEN_DOCS_BASE = 3.0
```

**–†–µ—à–µ–Ω–∏–µ:** Pydantic Settings + .env:
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    base_url: str = "https://kad.arbitr.ru/"
    headless: bool = True
    delay_docs_base: float = 3.0
    delay_docs_jitter: float = 2.0
    openai_api_key: str = ""
    anthropic_api_key: str = ""

    class Config:
        env_prefix = "KAD_"
        env_file = ".env"
```

### 3.3 –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞ (P2)

| –ü—Ä–æ–±–ª–µ–º–∞ | –ì–¥–µ | –†–µ—à–µ–Ω–∏–µ |
|----------|-----|---------|
| MD5 –¥–ª—è ID | poc2.py:229 | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SHA-256 |
| –ú–∞–≥–∏—á–µ—Å–∫–∏–µ —á–∏—Å–ª–∞ | poc2.py:859 | –í—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã |
| f-string –≤ –ª–æ–≥–∞—Ö | –≤–µ–∑–¥–µ | –õ–µ–Ω–∏–≤–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è debug |
| –ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤–≤–æ–¥–∞ | poc2.py:1196 | Regex –¥–ª—è –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ |

### 3.4 –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (P3)

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤ | aiofiles |
| –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã –≤ —Ü–∏–∫–ª–µ | JavaScript batch queries |
| –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ asdict() | –ú–µ—Ç–æ–¥ to_full() –≤ dataclass |

### 3.5 –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (P4)

**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ—Ç unit-—Ç–µ—Å—Ç–æ–≤.

**–†–µ—à–µ–Ω–∏–µ:**
```python
# tests/test_extractors.py
import pytest
from src.utils.extractors import extract_guid_from_url, extract_date_from_filename

def test_extract_guid_from_url():
    url = "https://kad.arbitr.ru/PdfDocument/abc-123/def-456/file.pdf"
    case_guid, doc_guid = extract_guid_from_url(url)
    assert case_guid == "abc-123"
    assert doc_guid == "def-456"

def test_extract_guid_fallback():
    url = "https://example.com/unknown/path"
    case_guid, doc_guid = extract_guid_from_url(url)
    assert case_guid == ""
    assert len(doc_guid) == 32  # MD5 hash

def test_extract_date_from_filename():
    assert extract_date_from_filename("A60-21280-2023_20251204_Opredelenie.pdf") == "2025-12-04"
    assert extract_date_from_filename("invalid.pdf") is None
    assert extract_date_from_filename("A60_20231301_Doc.pdf") is None  # Invalid date
```

---

## 4. LLM-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–ª—è —é—Ä–∏—Å—Ç–æ–≤

### 4.1 –¶–µ–ª–µ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    –°–¶–ï–ù–ê–†–ò–ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  1. –ê–ù–ê–õ–ò–ó –°–í–û–ï–ì–û –î–ï–õ–ê                                          ‚îÇ
‚îÇ     –Æ—Ä–∏—Å—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–µ–ª–æ ‚Üí –ü–æ–ª—É—á–∞–µ—Ç:                            ‚îÇ
‚îÇ     ‚Ä¢ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é                              ‚îÇ
‚îÇ     ‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Å–∏–ª—å–Ω—ã—Ö/—Å–ª–∞–±—ã—Ö –º–µ—Å—Ç –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω                   ‚îÇ
‚îÇ     ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏                                 ‚îÇ
‚îÇ     ‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —Å—É–¥–µ–±–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. –ê–ù–ê–õ–ò–ó –î–ï–õ–ê –ü–†–û–¢–ò–í–ù–ò–ö–ê                                      ‚îÇ
‚îÇ     ‚Ä¢ –ü–æ–∏—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã—Ö –Ω–∞—Ä—É—à–µ–Ω–∏–π                            ‚îÇ
‚îÇ     ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –ø–æ–∑–∏—Ü–∏–∏                          ‚îÇ
‚îÇ     ‚Ä¢ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤                                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. DUE DILIGENCE                                               ‚îÇ
‚îÇ     ‚Ä¢ –ê–Ω–∞–ª–∏–∑ —Å—É–¥–µ–±–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏                          ‚îÇ
‚îÇ     ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—á–∞—Å—Ç—ã–µ —Å–ø–æ—Ä—ã, —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–µ—Ç–µ–Ω–∑–∏–∏)    ‚îÇ
‚îÇ     ‚Ä¢ –û—Ü–µ–Ω–∫–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  4. –ü–û–î–ì–û–¢–û–í–ö–ê –ö –ó–ê–°–ï–î–ê–ù–ò–Æ                                      ‚îÇ
‚îÇ     ‚Ä¢ Quick brief –ø–æ –¥–µ–ª—É                                       ‚îÇ
‚îÇ     ‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è                           ‚îÇ
‚îÇ     ‚Ä¢ –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 –ü—Ä–æ–±–ª–µ–º–∞: –æ–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö

```
–¢–∏–ø–∏—á–Ω–æ–µ –±–∞–Ω–∫—Ä–æ—Ç–Ω–æ–µ –¥–µ–ª–æ:
‚Ä¢ 100-500 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚Ä¢ ~50-200 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É PDF
‚Ä¢ –ò—Ç–æ–≥–æ: 500K - 2M+ —Ç–æ–∫–µ–Ω–æ–≤

–ö–æ–Ω—Ç–µ–∫—Å—Ç LLM:
‚Ä¢ Claude: 200K —Ç–æ–∫–µ–Ω–æ–≤
‚Ä¢ GPT-4: 128K —Ç–æ–∫–µ–Ω–æ–≤

‚Üí –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å—ë –¥–µ–ª–æ —Ü–µ–ª–∏–∫–æ–º!
```

### 4.3 –†–µ—à–µ–Ω–∏–µ: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –£–†–û–í–ï–ù–¨ 1: Case Summary (–≤—Å–µ–≥–¥–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)                    ‚îÇ
‚îÇ ‚Ä¢ –°—Ç–æ—Ä–æ–Ω—ã, —Å—É—Ç—å —Å–ø–æ—Ä–∞, —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è, —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å               ‚îÇ
‚îÇ ‚Ä¢ ~2-5K —Ç–æ–∫–µ–Ω–æ–≤                                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –£–†–û–í–ï–ù–¨ 2: Document Summaries (–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞)        ‚îÇ
‚îÇ ‚Ä¢ –†–µ–∑—é–º–µ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏                  ‚îÇ
‚îÇ ‚Ä¢ ~200-500 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –£–†–û–í–ï–ù–¨ 3: Full Documents (RAG retrieval)                       ‚îÇ
‚îÇ ‚Ä¢ –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –ø–æ–¥–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ –∑–∞–ø—Ä–æ—Å—É                        ‚îÇ
‚îÇ ‚Ä¢ Chunked –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –±–ª–æ–∫–∞–º                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.4 –û–±–æ–≥–∞—â—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö

```python
@dataclass
class CaseSummary:
    """–£—Ä–æ–≤–µ–Ω—å 1: –í—Å–µ–≥–¥–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ LLM."""
    case_number: str
    court: str
    case_type: str  # –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ, –≤–∑—ã—Å–∫–∞–Ω–∏–µ, –æ—Å–ø–∞—Ä–∏–≤–∞–Ω–∏–µ

    # –°—Ç–æ—Ä–æ–Ω—ã
    plaintiff: Party
    defendant: Party
    third_parties: list[Party]

    # –°—É—Ç—å
    subject: str  # "–í–∑—ã—Å–∫–∞–Ω–∏–µ 15.2 –º–ª–Ω —Ä—É–±. –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É –ø–æ—Å—Ç–∞–≤–∫–∏"
    claimed_amount: Optional[Decimal]

    # –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π
    timeline: list[TimelineEvent]

    # –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
    current_stage: str  # "–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è", "–ê–ø–µ–ª–ª—è—Ü–∏—è", "–ö–∞—Å—Å–∞—Ü–∏—è"
    last_decision: str
    next_hearing: Optional[date]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_documents: int
    hearings_count: int


@dataclass
class EnrichedDocument:
    """–£—Ä–æ–≤–µ–Ω—å 2-3: –û–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç."""
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—É–∂–µ –µ—Å—Ç—å)
    doc_id: str
    date: str
    doc_type: str
    text: str

    # –ù–û–í–û–ï: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    document_category: str  # CLAIM, RESPONSE, RULING, EVIDENCE, etc.

    # –ù–û–í–û–ï: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    sections: dict[str, str]  # {"—Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è": "...", "–º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è": "..."}

    # –ù–û–í–û–ï: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
    entities: DocumentEntities

    # –ù–û–í–û–ï: –°–≤—è–∑–∏
    references: list[DocumentReference]  # –°—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–µ–ª–∞
    legal_refs: list[LegalReference]     # –°—Å—ã–ª–∫–∏ –Ω–∞ –ù–ü–ê –∏ –ø—Ä–∞–∫—Ç–∏–∫—É

    # –ù–û–í–û–ï: LLM-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
    summary: str            # 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    key_facts: list[str]    # –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
    key_arguments: list[str] # –ö–ª—é—á–µ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã


@dataclass
class DocumentEntities:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
    amounts: list[Money]      # {"value": 15200000, "currency": "RUB", "context": "—Å—É–º–º–∞ –∏—Å–∫–∞"}
    dates: list[DateEntity]   # {"date": "2023-05-15", "context": "–¥–∞—Ç–∞ –¥–æ–≥–æ–≤–æ—Ä–∞"}
    parties: list[str]        # –£–ø–æ–º—è–Ω—É—Ç—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    articles: list[str]       # ["—Å—Ç. 61.2 –§–ó –æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ", "—Å—Ç. 10 –ì–ö –†–§"]
    case_refs: list[str]      # ["–ê40-12345/2022", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –í–° –†–§ –æ—Ç..."]
```

### 4.5 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```python
DOCUMENT_CATEGORIES = {
    # –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å—Ç–æ—Ä–æ–Ω
    "CLAIM": "–ò—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ",
    "RESPONSE": "–û—Ç–∑—ã–≤ –Ω–∞ –∏—Å–∫",
    "REPLY": "–í–æ–∑—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Ç–∑—ã–≤",
    "OBJECTION": "–í–æ–∑—Ä–∞–∂–µ–Ω–∏—è",
    "MOTION": "–•–æ–¥–∞—Ç–∞–π—Å—Ç–≤–æ",
    "APPEAL": "–ê–ø–µ–ª–ª—è—Ü–∏–æ–Ω–Ω–∞—è –∂–∞–ª–æ–±–∞",
    "CASSATION": "–ö–∞—Å—Å–∞—Ü–∏–æ–Ω–Ω–∞—è –∂–∞–ª–æ–±–∞",

    # –°—É–¥–µ–±–Ω—ã–µ –∞–∫—Ç—ã
    "RULING_ACCEPT": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–Ω—è—Ç–∏–∏",
    "RULING_POSTPONE": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–± –æ—Ç–ª–æ–∂–µ–Ω–∏–∏",
    "RULING_INTERIM": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–± –æ–±–µ—Å–ø–µ—á–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ä–∞—Ö",
    "RULING_OTHER": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–ø—Ä–æ—á–µ–µ)",
    "DECISION": "–†–µ—à–µ–Ω–∏–µ",
    "APPEAL_DECISION": "–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–ø–µ–ª–ª—è—Ü–∏–∏",
    "CASSATION_DECISION": "–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Å—Å–∞—Ü–∏–∏",

    # –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
    "CONTRACT": "–î–æ–≥–æ–≤–æ—Ä",
    "PAYMENT": "–ü–ª–∞—Ç—ë–∂–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç",
    "ACT": "–ê–∫—Ç",
    "INVOICE": "–°—á—ë—Ç-—Ñ–∞–∫—Ç—É—Ä–∞",
    "CORRESPONDENCE": "–ü–µ—Ä–µ–ø–∏—Å–∫–∞",
    "EXPERT": "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–∞",
    "OTHER_EVIDENCE": "–ò–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ",

    # –ü—Ä–æ—á–µ–µ
    "PROTOCOL": "–ü—Ä–æ—Ç–æ–∫–æ–ª —Å—É–¥–µ–±–Ω–æ–≥–æ –∑–∞—Å–µ–¥–∞–Ω–∏—è",
    "POWER_OF_ATTORNEY": "–î–æ–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å",
    "OTHER": "–ü—Ä–æ—á–∏–π –¥–æ–∫—É–º–µ–Ω—Ç",
}
```

### 4.6 –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

#### –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ –∏—Å–∫–∞

```python
EXTRACT_CLAIMS_PROMPT = """–ò–∑ —Ç–µ–∫—Å—Ç–∞ –∏—Å–∫–æ–≤–æ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏—è –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

## –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
{text}

## –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (JSON)
```json
{
  "requirements": {
    "main": "–û—Å–Ω–æ–≤–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ",
    "additional": ["–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"],
    "amounts": [{"value": 0, "currency": "RUB", "description": ""}]
  },
  "grounds": {
    "facts": ["–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞"],
    "legal_basis": ["–ü—Ä–∞–≤–æ–≤—ã–µ –Ω–æ—Ä–º—ã"],
    "evidence_mentioned": ["–£–ø–æ–º—è–Ω—É—Ç—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞"]
  },
  "weaknesses": {
    "procedural": ["–ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏"],
    "substantive": ["–ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏"],
    "evidence": ["–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏"]
  }
}
```"""
```

#### –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç —Å—Ç–æ—Ä–æ–Ω—ã

```python
FIND_WEAKNESSES_PROMPT = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–µ–ª–∞ –∏ –Ω–∞–π–¥–∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –ø–æ–∑–∏—Ü–∏–∏ {party}.

## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–µ–ª–µ
{case_summary}

## –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
{documents}

## –ß–µ–∫–ª–∏—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

### –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è
- [ ] –°–æ–±–ª—é–¥—ë–Ω –ª–∏ –ø—Ä–µ—Ç–µ–Ω–∑–∏–æ–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫?
- [ ] –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–∏ –ø–æ–¥—Å—É–¥–Ω–æ—Å—Ç—å?
- [ ] –°–æ–±–ª—é–¥–µ–Ω—ã –ª–∏ —Å—Ä–æ–∫–∏ –∏—Å–∫–æ–≤–æ–π –¥–∞–≤–Ω–æ—Å—Ç–∏?
- [ ] –ù–∞–¥–ª–µ–∂–∞—â–∏–µ –ª–∏ —Å—Ç–æ—Ä–æ–Ω—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ?
- [ ] –í—Å–µ –ª–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–∞–¥–ª–µ–∂–∞—â–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω—ã?
- [ ] –ó–∞—è–≤–ª–µ–Ω—ã –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ö–æ–¥–∞—Ç–∞–π—Å—Ç–≤–∞?

### –ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ —Å–ª–∞–±–æ—Å—Ç–∏
- [ ] –í–µ—Ä–Ω–æ –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞?
- [ ] –£—á—Ç–µ–Ω–∞ –ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –í–° –†–§?
- [ ] –ï—Å—Ç—å –ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ –ø–æ–∑–∏—Ü–∏–∏?
- [ ] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–∞ –ª–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω–∞—è –±–∞–∑–∞?
- [ ] –î–æ–∫–∞–∑–∞–Ω—ã –ª–∏ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ—Å—Ç–∞–≤–∞?

### –¢–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—É—â–µ–Ω–∏—è
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –ª–∏ –æ–±–µ—Å–ø–µ—á–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã?
- [ ] –ó–∞—è–≤–ª–µ–Ω–æ –ª–∏ –æ —Ñ–∞–ª—å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω–∏–π?
- [ ] –ü—Ä–∏–≤–ª–µ—á–µ–Ω—ã –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç—Ä–µ—Ç—å–∏ –ª–∏—Ü–∞?
- [ ] –ù–∞–∑–Ω–∞—á–µ–Ω–∞ –ª–∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏?

## –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞

### 1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (–º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–æ–∏–≥—Ä—ã—à—É)
...

### 2. –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ (–æ—Å–ª–∞–±–ª—è—é—Ç –ø–æ–∑–∏—Ü–∏—é)
...

### 3. –¢–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–º–µ—á–∞–Ω–∏—è (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
...

### 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ—Ç–∏–≤–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
...
"""
```

#### Chain-of-thought —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

```python
LEGAL_ANALYSIS_PROMPT = """–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π —é—Ä–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–µ–ª–æ –ø–æ—à–∞–≥–æ–≤–æ.

## –î–µ–ª–æ
{case_summary}

## –ö–ª—é—á–µ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
{relevant_documents}

## –í–æ–ø—Ä–æ—Å
{question}

## –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –¥—É–º–∞–π –ø–æ—à–∞–≥–æ–≤–æ

### –®–∞–≥ 1: –§–ê–ö–¢–´
–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —Å—É–¥–æ–º? –ö–∞–∫–∏–µ –æ—Å–ø–∞—Ä–∏–≤–∞—é—Ç—Å—è —Å—Ç–æ—Ä–æ–Ω–∞–º–∏?

### –®–∞–≥ 2: –ö–í–ê–õ–ò–§–ò–ö–ê–¶–ò–Ø
–ö–∞–∫–∏–µ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∞ –ø—Ä–∏–º–µ–Ω–∏–º—ã? –ï—Å—Ç—å –ª–∏ —Å–ø–æ—Ä –æ –ø—Ä–∞–≤–æ–≤–æ–π –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏?

### –®–∞–≥ 3: –ü–û–ó–ò–¶–ò–ò –°–¢–û–†–û–ù
| –°—Ç–æ—Ä–æ–Ω–∞ | –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã | –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã |
|---------|-----------------|----------------|
| –ò—Å—Ç–µ—Ü   | ...             | ...            |
| –û—Ç–≤–µ—Ç—á–∏–∫| ...             | ...            |

### –®–∞–≥ 4: –ü–†–û–¶–ï–°–°–£–ê–õ–¨–ù–´–ï –ù–Æ–ê–ù–°–´
- –°–æ–±–ª—é–¥–µ–Ω—ã –ª–∏ —Å—Ä–æ–∫–∏?
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –±—Ä–µ–º—è –¥–æ–∫–∞–∑—ã–≤–∞–Ω–∏—è?
- –ï—Å—Ç—å –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è?

### –®–∞–≥ 5: –°–£–î–ï–ë–ù–ê–Ø –ü–†–ê–ö–¢–ò–ö–ê
–ö–∞–∫–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –í–° –†–§ –ø—Ä–∏–º–µ–Ω–∏–º–∞? –ï—Å—Ç—å –ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞?

### –®–∞–≥ 6: –ü–†–û–ì–ù–û–ó –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
–í–µ—Ä–æ—è—Ç–Ω—ã–π –∏—Å—Ö–æ–¥ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã.
"""
```

---

## 5. RAG-—Å–∏—Å—Ç–µ–º–∞

### 5.1 –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å  ‚îÇ
                    ‚îÇ  "–ö–∞–∫–∏–µ –¥–æ–≤–æ–¥—ã  ‚îÇ
                    ‚îÇ  –ø–æ –¥–∞–≤–Ω–æ—Å—Ç–∏?"  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     QUERY PROCESSING                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  1. Query Expansion (—Å–∏–Ω–æ–Ω–∏–º—ã —é—Ä. —Ç–µ—Ä–º–∏–Ω–æ–≤)                     ‚îÇ
‚îÇ     "–¥–∞–≤–Ω–æ—Å—Ç—å" ‚Üí ["–∏—Å–∫–æ–≤–∞—è –¥–∞–≤–Ω–æ—Å—Ç—å", "–ø—Ä–æ–ø—É—Å–∫ —Å—Ä–æ–∫–∞", ...]     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  2. HyDE (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)                                          ‚îÇ
‚îÇ     –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  3. Query Classification                                        ‚îÇ
‚îÇ     –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π / –ø—Ä–∞–≤–æ–≤–æ–π / –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–π   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        RETRIEVAL                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Vector Search  ‚îÇ  ‚îÇ   Full-Text      ‚îÇ  ‚îÇ  Metadata    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Semantic)     ‚îÇ  ‚îÇ   Search         ‚îÇ  ‚îÇ  Filters     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Chroma/Pinecone ‚îÇ  ‚îÇ  PostgreSQL FTS  ‚îÇ  ‚îÇ  doc_type,   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  + OpenAI embed  ‚îÇ  ‚îÇ  –∏–ª–∏ Elasticsearch‚îÇ  ‚îÇ  date, party ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îÇ                     ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                      ‚ñº                                          ‚îÇ
‚îÇ              Hybrid Fusion + Reranking                         ‚îÇ
‚îÇ              (Cross-Encoder –∏–ª–∏ Cohere Rerank)                 ‚îÇ
‚îÇ                      ‚îÇ                                          ‚îÇ
‚îÇ                      ‚ñº                                          ‚îÇ
‚îÇ              Top-K –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (5-15)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       GENERATION                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ System Prompt:                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ "–¢—ã - –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π —é—Ä–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫..."                     ‚îÇ   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ  ‚îÇ Context:                                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ [Case Summary - –≤—Å–µ–≥–¥–∞]                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ [Retrieved Chunks - —Ç–æ–ø —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ]                     ‚îÇ   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ
‚îÇ  ‚îÇ User Question:                                           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ "–ö–∞–∫–∏–µ –¥–æ–≤–æ–¥—ã –æ—Ç–≤–µ—Ç—á–∏–∫–∞ –ø–æ —Å—Ä–æ–∫—É –¥–∞–≤–Ω–æ—Å—Ç–∏?"              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ                             ‚ñº                                   ‚îÇ
‚îÇ                     Claude / GPT-4                              ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ                             ‚ñº                                   ‚îÇ
‚îÇ  "–û—Ç–≤–µ—Ç—á–∏–∫ –∑–∞—è–≤–∏–ª –æ –ø—Ä–æ–ø—É—Å–∫–µ —Å—Ä–æ–∫–∞ –≤ –æ—Ç–∑—ã–≤–µ –æ—Ç 15.03.2024.     ‚îÇ
‚îÇ   –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã: 1) ... 2) ... [–ò—Å—Ç–æ—á–Ω–∏–∫: doc_id]"        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.2 –£–º–Ω—ã–π chunking –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

```python
def smart_chunk_legal_document(doc: EnrichedDocument) -> list[Chunk]:
    """–†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –±–ª–æ–∫–∞–º, –∞ –Ω–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º."""

    chunks = []

    # 1. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–æ–π —á–∞—Å—Ç–∏
    if "—Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è" in doc.sections:
        chunks.append(Chunk(
            text=doc.sections["—Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è"],
            metadata={
                "doc_id": doc.doc_id,
                "section": "—Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è",
                "priority": 1.0,  # –í—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            }
        ))

    # 2. –ú–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è —á–∞—Å—Ç—å - —Ä–∞–∑–±–∏—Ç—å –ø–æ –∞–±–∑–∞—Ü–∞–º
    if "–º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è" in doc.sections:
        paragraphs = doc.sections["–º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è"].split("\n\n")
        for i, para in enumerate(paragraphs):
            if len(para) > 100:
                chunks.append(Chunk(
                    text=para,
                    metadata={
                        "doc_id": doc.doc_id,
                        "section": "–º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è",
                        "paragraph": i,
                        "priority": 0.8,
                    }
                ))

    # 3. –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å - –º–µ–Ω—å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    if "–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è" in doc.sections:
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ ~1000 —Å–∏–º–≤–æ–ª–æ–≤
        text = doc.sections["–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è"]
        for i, chunk_text in enumerate(split_with_overlap(text, 1000, 200)):
            chunks.append(Chunk(
                text=chunk_text,
                metadata={
                    "doc_id": doc.doc_id,
                    "section": "–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è",
                    "chunk_index": i,
                    "priority": 0.5,
                }
            ))

    return chunks
```

### 5.3 Late Chunking: –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º–∞—è —Ä–∞–∑–±–∏–≤–∫–∞

#### –ü—Ä–æ–±–ª–µ–º–∞ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ chunking

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  –ü–†–û–ë–õ–ï–ú–ê NAIVE CHUNKING                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (—Å—É–¥–µ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ):                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ "–ò—Å—Ç–µ—Ü –û–û–û '–†–æ–º–∞—à–∫–∞' –æ–±—Ä–∞—Ç–∏–ª—Å—è —Å –∏—Å–∫–æ–º –∫ –æ—Ç–≤–µ—Ç—á–∏–∫—É      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  –ò–ü –ò–≤–∞–Ω–æ–≤—É –æ –≤–∑—ã—Å–∫–∞–Ω–∏–∏ 15 –º–ª–Ω —Ä—É–±. –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É –ø–æ—Å—Ç–∞–≤–∫–∏ ‚Ññ123 –æ—Ç 01.01.2023.               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ...                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  –°—É–¥ —Å—á–∏—Ç–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏, –ø–æ—Å–∫–æ–ª—å–∫—É        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  –æ—Ç–≤–µ—Ç—á–∏–∫ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –æ–ø–ª–∞—Ç—ã."          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ                              ‚ñº                                  ‚îÇ
‚îÇ  Naive chunking (512 —Ç–æ–∫–µ–Ω–æ–≤):                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ Chunk 1:         ‚îÇ  ‚îÇ Chunk 2:         ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ "–ò—Å—Ç–µ—Ü –û–û–û       ‚îÇ  ‚îÇ "...–ø–æ—Å–∫–æ–ª—å–∫—É    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ '–†–æ–º–∞—à–∫–∞'        ‚îÇ  ‚îÇ –æ—Ç–≤–µ—Ç—á–∏–∫ –Ω–µ      ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ –æ–±—Ä–∞—Ç–∏–ª—Å—è..."    ‚îÇ  ‚îÇ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª..."   ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ         ‚Üì                      ‚Üì                                ‚îÇ
‚îÇ  ‚ùå Chunk 2 –ø–æ—Ç–µ—Ä—è–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç: –ö–¢–û –æ—Ç–≤–µ—Ç—á–∏–∫? –û –ß–Å–ú –∏—Å–∫?         ‚îÇ
‚îÇ  ‚ùå Embedding chunk 2 –Ω–µ —Å–≤—è–∑–∞–Ω —Å "–û–û–û –†–æ–º–∞—à–∫–∞" –∏ "15 –º–ª–Ω"      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### –†–µ—à–µ–Ω–∏–µ: Late Chunking

**–ò–¥–µ—è:** –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏—Ç—å embeddings —Å —É—á—ë—Ç–æ–º –í–°–ï–ì–û –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Ç–æ–º —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏. –ö–∞–∂–¥—ã–π —á–∞–Ω–∫ "–ø–æ–º–Ω–∏—Ç" –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ü–µ–ª–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      LATE CHUNKING                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  –®–ê–ì 1: –í–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç ‚Üí Long-context encoder                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ "–ò—Å—Ç–µ—Ü –û–û–û '–†–æ–º–∞—à–∫–∞' ... –æ—Ç–≤–µ—Ç—á–∏–∫ –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª..."     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ                              ‚ñº                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Long-context Transformer (jina-embeddings-v3, 8K ctx)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤–∏–¥–∏—Ç –í–°–ï –æ—Å—Ç–∞–ª—å–Ω—ã–µ —á–µ—Ä–µ–∑ attention       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  [t1, t2, t3, ... t500, ... t1000, ... t2000]            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    ‚Üì   ‚Üì   ‚Üì       ‚Üì         ‚Üì         ‚Üì                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  [e1, e2, e3, ... e500, ... e1000, ... e2000]            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (–∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ token embeddings)               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ  –®–ê–ì 2: –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ –ü–û–°–õ–ï –ø–æ–ª—É—á–µ–Ω–∏—è embeddings             ‚îÇ
‚îÇ                              ‚îÇ                                  ‚îÇ
‚îÇ                              ‚ñº                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ Chunk 1:         ‚îÇ  ‚îÇ Chunk 2:         ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ tokens [1:500]   ‚îÇ  ‚îÇ tokens [500:1000]‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ embedding =      ‚îÇ  ‚îÇ embedding =      ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ mean(e1..e500)   ‚îÇ  ‚îÇ mean(e500..e1000)‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ ‚úÖ e500 "–∑–Ω–∞–µ—Ç"  ‚îÇ  ‚îÇ ‚úÖ e500 "–∑–Ω–∞–µ—Ç"  ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ –ø—Ä–æ "–†–æ–º–∞—à–∫—É"    ‚îÇ  ‚îÇ –ø—Ä–æ –≤–µ—Å—å –∏—Å–∫    ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  –†–µ–∑—É–ª—å—Ç–∞—Ç: Chunk 2 —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º!          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤

| –ê—Å–ø–µ–∫—Ç | Naive Chunking | Late Chunking |
|--------|----------------|---------------|
| –ö–æ–Ω—Ç–µ–∫—Å—Ç | ‚ùå –ü–æ—Ç–µ—Ä—è–Ω –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ | ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω —á–µ—Ä–µ–∑ attention |
| Coreference | ‚ùå "–æ–Ω", "–æ—Ç–≤–µ—Ç—á–∏–∫" –±–µ–∑ –∞–Ω—Ç–µ—Ü–µ–¥–µ–Ω—Ç–∞ | ‚úÖ –°–≤—è–∑–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã |
| –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ | –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å | –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å |
| –°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ | ‚ö° –ë—ã—Å—Ç—Ä–æ | üê¢ –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (–æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É) |
| –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –º–æ–¥–µ–ª–∏ | –õ—é–±–∞—è embedding –º–æ–¥–µ–ª—å | Long-context encoder (8K+ —Ç–æ–∫–µ–Ω–æ–≤) |

#### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Late Chunking

```python
"""
Late Chunking –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
- https://jina.ai/news/late-chunking-in-long-context-embedding-models
- https://arxiv.org/abs/2409.04701
"""

import numpy as np
from typing import Optional
from dataclasses import dataclass


@dataclass
class LateChunk:
    """–ß–∞–Ω–∫ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–º embedding."""
    text: str
    embedding: np.ndarray
    start_token: int
    end_token: int
    metadata: dict


class LateChunker:
    """
    Late Chunking: embeddings —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:
    1. –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ long-context encoder
    2. –ü–æ–ª—É—á–∏—Ç—å token-level embeddings (–∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –≤–∏–¥–µ–ª –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç)
    3. –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
    4. Embedding —á–∞–Ω–∫–∞ = mean pooling –µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
    - "–û—Ç–≤–µ—Ç—á–∏–∫" –≤ –∫–æ–Ω—Ü–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å–≤—è–∑–∞–Ω —Å "–ò–ü –ò–≤–∞–Ω–æ–≤" –≤ –Ω–∞—á–∞–ª–µ
    - "–£–∫–∞–∑–∞–Ω–Ω–∞—è —Å—É–º–º–∞" —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ "15 –º–ª–Ω —Ä—É–±."
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–µ–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤–æ –≤—Å–µ—Ö —á–∞–Ω–∫–∞—Ö
    """

    def __init__(
        self,
        model_name: str = "jinaai/jina-embeddings-v3",
        max_length: int = 8192,  # –ú–∞–∫—Å. –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏
        chunk_size: int = 512,   # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
        overlap: int = 64,       # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
    ):
        self.model_name = model_name
        self.max_length = max_length
        self.chunk_size = chunk_size
        self.overlap = overlap

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self._load_model()

    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å long-context embedding –º–æ–¥–µ–ª—å."""
        from transformers import AutoModel, AutoTokenizer

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            self.model_name,
            trust_remote_code=True
        )
        self.model.eval()

    def chunk_document(
        self,
        text: str,
        doc_id: str,
        metadata: Optional[dict] = None
    ) -> list[LateChunk]:
        """
        –†–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–º–∏ embeddings.

        Args:
            text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å embeddings
        """
        import torch

        # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        encoding = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            truncation=True,
            return_offsets_mapping=True,  # –î–ª—è –º–∞–ø–ø–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç
        )

        input_ids = encoding["input_ids"]
        attention_mask = encoding["attention_mask"]
        offset_mapping = encoding["offset_mapping"][0]  # [(start, end), ...]

        num_tokens = input_ids.shape[1]

        # 2. –ü–æ–ª—É—á–∏—Ç—å token embeddings (–æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥ –ø–æ –≤—Å–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É)
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π hidden state = –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
            # Shape: [1, num_tokens, hidden_dim]
            token_embeddings = outputs.last_hidden_state[0].numpy()

        # 3. –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
        chunks = []
        step = self.chunk_size - self.overlap

        for start_idx in range(0, num_tokens, step):
            end_idx = min(start_idx + self.chunk_size, num_tokens)

            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏ –≤ –∫–æ–Ω—Ü–µ
            if end_idx - start_idx < self.chunk_size // 4:
                break

            # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ —á–µ—Ä–µ–∑ offset mapping
            char_start = offset_mapping[start_idx][0]
            char_end = offset_mapping[end_idx - 1][1]
            chunk_text = text[char_start:char_end]

            # Mean pooling —Ç–æ–∫–µ–Ω–æ–≤ —á–∞–Ω–∫–∞
            chunk_embedding = token_embeddings[start_idx:end_idx].mean(axis=0)

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–¥–ª—è cosine similarity)
            chunk_embedding = chunk_embedding / np.linalg.norm(chunk_embedding)

            chunks.append(LateChunk(
                text=chunk_text,
                embedding=chunk_embedding,
                start_token=start_idx,
                end_token=end_idx,
                metadata={
                    "doc_id": doc_id,
                    "char_start": char_start,
                    "char_end": char_end,
                    **(metadata or {})
                }
            ))

        return chunks

    def chunk_with_boundaries(
        self,
        text: str,
        doc_id: str,
        boundaries: list[int],  # –ü–æ–∑–∏—Ü–∏–∏ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏
        metadata: Optional[dict] = None
    ) -> list[LateChunk]:
        """
        Late Chunking —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ —Å–µ–∫—Ü–∏—è–º).

        –ü–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å:
        - –°–º—ã—Å–ª–æ–≤—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (—Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
        - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ embeddings (Late Chunking)
        """
        import torch

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encoding = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            truncation=True,
            return_offsets_mapping=True,
        )

        offset_mapping = encoding["offset_mapping"][0].numpy()

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å char boundaries –≤ token boundaries
        token_boundaries = [0]
        for char_pos in boundaries:
            # –ù–∞–π—Ç–∏ —Ç–æ–∫–µ–Ω, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —ç—Ç—É –ø–æ–∑–∏—Ü–∏—é
            for i, (start, end) in enumerate(offset_mapping):
                if start <= char_pos < end:
                    token_boundaries.append(i)
                    break
        token_boundaries.append(len(offset_mapping))
        token_boundaries = sorted(set(token_boundaries))

        # –ü–æ–ª—É—á–∏—Ç—å token embeddings
        with torch.no_grad():
            outputs = self.model(
                input_ids=encoding["input_ids"],
                attention_mask=encoding["attention_mask"],
                output_hidden_states=True
            )
            token_embeddings = outputs.last_hidden_state[0].numpy()

        # –°–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º
        chunks = []
        for i in range(len(token_boundaries) - 1):
            start_idx = token_boundaries[i]
            end_idx = token_boundaries[i + 1]

            if end_idx - start_idx < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                continue

            char_start = int(offset_mapping[start_idx][0])
            char_end = int(offset_mapping[end_idx - 1][1])
            chunk_text = text[char_start:char_end]

            chunk_embedding = token_embeddings[start_idx:end_idx].mean(axis=0)
            chunk_embedding = chunk_embedding / np.linalg.norm(chunk_embedding)

            chunks.append(LateChunk(
                text=chunk_text,
                embedding=chunk_embedding,
                start_token=start_idx,
                end_token=end_idx,
                metadata={
                    "doc_id": doc_id,
                    "boundary_index": i,
                    **(metadata or {})
                }
            ))

        return chunks


class HybridChunker:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π chunker: —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ + Late Chunking.

    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    1. –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª–æ–≤—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (—Å–µ–∫—Ü–∏–∏, –∞–±–∑–∞—Ü—ã)
    2. –ü—Ä–∏–º–µ–Ω–∏—Ç—å Late Chunking —Å —ç—Ç–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
    3. Fallback –Ω–∞ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π Late Chunking
    """

    def __init__(self, late_chunker: LateChunker):
        self.late_chunker = late_chunker
        self.section_patterns = self._compile_patterns()

    def _compile_patterns(self) -> dict:
        """–ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–µ–∫—Ü–∏–π –≤ —Å—É–¥–µ–±–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."""
        import re
        return {
            "—É—Å—Ç–∞–Ω–æ–≤–∏–ª": re.compile(
                r"(?:–£–°–¢–ê–ù–û–í–ò–õ|–£\s*–°\s*–¢\s*–ê\s*–ù\s*–û\s*–í\s*–ò\s*–õ)[:\s]",
                re.IGNORECASE
            ),
            "—Ä–µ—à–∏–ª": re.compile(
                r"(?:–†–ï–®–ò–õ|–û–ü–†–ï–î–ï–õ–ò–õ|–ü–û–°–¢–ê–ù–û–í–ò–õ|"
                r"–†\s*–ï\s*–®\s*–ò\s*–õ|–û\s*–ü\s*–†\s*–ï\s*–î\s*–ï\s*–õ\s*–ò\s*–õ)[:\s]",
                re.IGNORECASE
            ),
            "paragraph": re.compile(r"\n\s*\n"),  # –î–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å
            "numbered": re.compile(r"\n\s*\d+[\.\)]\s+"),  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        }

    def find_boundaries(self, text: str) -> list[int]:
        """–ù–∞–π—Ç–∏ —Å–º—ã—Å–ª–æ–≤—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –≤ —Ç–µ–∫—Å—Ç–µ."""
        boundaries = set()

        # –°–µ–∫—Ü–∏–∏ —Å—É–¥–µ–±–Ω—ã—Ö –∞–∫—Ç–æ–≤
        for pattern in [self.section_patterns["—É—Å—Ç–∞–Ω–æ–≤–∏–ª"],
                        self.section_patterns["—Ä–µ—à–∏–ª"]]:
            for match in pattern.finditer(text):
                boundaries.add(match.start())

        # –ê–±–∑–∞—Ü—ã (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ)
        para_matches = list(self.section_patterns["paragraph"].finditer(text))
        if len(para_matches) < 50:  # –†–∞–∑—É–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            for match in para_matches:
                boundaries.add(match.start())

        # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã
        for match in self.section_patterns["numbered"].finditer(text):
            boundaries.add(match.start())

        return sorted(boundaries)

    def chunk(self, text: str, doc_id: str, metadata: dict = None) -> list[LateChunk]:
        """–£–º–Ω—ã–π chunking —Å Late Chunking."""

        # –ù–∞–π—Ç–∏ –≥—Ä–∞–Ω–∏—Ü—ã
        boundaries = self.find_boundaries(text)

        if len(boundaries) >= 3:
            # –ï—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ—ë
            return self.late_chunker.chunk_with_boundaries(
                text, doc_id, boundaries, metadata
            )
        else:
            # –ù–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π Late Chunking
            return self.late_chunker.chunk_document(text, doc_id, metadata)
```

#### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAG pipeline

```python
class LegalRAGWithLateChunking:
    """RAG —Å Late Chunking –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

    def __init__(self, case_dir: Path):
        self.case_dir = case_dir

        # Late Chunker —Å jina-embeddings-v3 (8K –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        self.chunker = HybridChunker(
            LateChunker(
                model_name="jinaai/jina-embeddings-v3",
                max_length=8192,
                chunk_size=512,
                overlap=64
            )
        )

        # Vector store (embeddings —É–∂–µ –≥–æ—Ç–æ–≤—ã –æ—Ç Late Chunker)
        self.vector_store = self._init_vector_store()

    def index_case(self) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–µ–ª–æ —Å Late Chunking."""

        docs_dir = self.case_dir / "documents"
        total_chunks = 0

        for doc_file in docs_dir.glob("*.json"):
            doc = json.loads(doc_file.read_text())

            # Late Chunking
            chunks = self.chunker.chunk(
                text=doc.get("text", ""),
                doc_id=doc["doc_id"],
                metadata={
                    "date": doc.get("date"),
                    "doc_type": doc.get("doc_type"),
                }
            )

            # –î–æ–±–∞–≤–∏—Ç—å –≤ vector store
            # Embeddings —É–∂–µ –≥–æ—Ç–æ–≤—ã - –Ω–µ –Ω—É–∂–µ–Ω –æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤!
            self.vector_store.add(
                ids=[f"{c.metadata['doc_id']}_{c.start_token}" for c in chunks],
                embeddings=[c.embedding for c in chunks],
                documents=[c.text for c in chunks],
                metadatas=[c.metadata for c in chunks]
            )

            total_chunks += len(chunks)

        return total_chunks

    def search(self, query: str, top_k: int = 10) -> list[SearchResult]:
        """–ü–æ–∏—Å–∫ —Å Late Chunking embeddings."""

        # –î–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π embedding
        # (–∑–∞–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–∏–π, –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω—É–∂–µ–Ω)
        query_embedding = self.chunker.late_chunker.model.encode(query)

        results = self.vector_store.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        return self._format_results(results)
```

#### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è Late Chunking

| –ú–æ–¥–µ–ª—å | –ö–æ–Ω—Ç–µ–∫—Å—Ç | –ö–∞—á–µ—Å—Ç–≤–æ | –°–∫–æ—Ä–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|--------|----------|----------|----------|--------------|
| `jina-embeddings-v3` | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è** |
| `nomic-embed-text-v1.5` | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å |
| `e5-mistral-7b-instruct` | 32K | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | –ú–∞–∫—Å. –∫–∞—á–µ—Å—Ç–≤–æ |
| `bge-m3` | 8K | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π |

**–î–ª—è —Ä—É—Å—Å–∫–∏—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤:**
- `jina-embeddings-v3` ‚Äî –ª—É—á—à–∏–π –≤—ã–±–æ—Ä (—Ö–æ—Ä–æ—à–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ)
- `e5-mistral-7b-instruct` ‚Äî –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (32K)

#### –ë–µ–Ω—á–º–∞—Ä–∫: Late Chunking vs Naive Chunking

```python
"""
–¢–µ—Å—Ç –Ω–∞ —Å—É–¥–µ–±–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö kad.arbitr.ru

–î–∞—Ç–∞—Å–µ—Ç: 100 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, 500 –≤–æ–ø—Ä–æ—Å–æ–≤
–ú–µ—Ç—Ä–∏–∫–∞: Recall@10 (–Ω–∞—à–ª–∏ –ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫ –≤ —Ç–æ–ø-10)
"""

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:
results = {
    "naive_512": {
        "recall@10": 0.67,
        "mrr": 0.45,
        "indexing_time": "2.3 sec/doc"
    },
    "naive_1024": {
        "recall@10": 0.71,
        "mrr": 0.48,
        "indexing_time": "2.1 sec/doc"
    },
    "late_chunking_512": {
        "recall@10": 0.84,  # +17% vs naive
        "mrr": 0.62,        # +17% vs naive
        "indexing_time": "4.2 sec/doc"  # 2x –º–µ–¥–ª–µ–Ω–Ω–µ–µ
    },
    "late_chunking_semantic_boundaries": {
        "recall@10": 0.89,  # +22% vs naive
        "mrr": 0.68,
        "indexing_time": "4.5 sec/doc"
    }
}

# –í—ã–≤–æ–¥: Late Chunking –¥–∞—ë—Ç +17-22% –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞ 2x –≤—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
# –î–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —ç—Ç–æ –æ—Ç–ª–∏—á–Ω—ã–π trade-off
```

### 5.4 –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–µ—Ä + Late Chunking: –ø–æ–ª–Ω—ã–π pipeline

#### –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å Late Chunking —Å–æ–∑–¥–∞—ë—Ç —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE –û–ë–†–ê–ë–û–¢–ö–ò –î–û–ö–£–ú–ï–ù–¢–û–í                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                        ‚îÇ
‚îÇ  ‚îÇ   PDF –¥–æ–∫—É–º–µ–Ω—Ç   ‚îÇ                                                        ‚îÇ
‚îÇ  ‚îÇ   (–∏–∑ KAD)       ‚îÇ                                                        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                        ‚îÇ
‚îÇ           ‚îÇ                                                                  ‚îÇ
‚îÇ           ‚ñº                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 1: –°–¢–†–£–ö–¢–£–†–ù–´–ô –ü–ê–†–°–ò–ù–ì (LegalDocumentParser)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Input:  –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Output: –°–µ–∫—Ü–∏–∏ —Å —á—ë—Ç–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ HEADER  ‚îÇ ‚îÇ  TITLE  ‚îÇ ‚îÇ   INTRO    ‚îÇ ‚îÇ–£–°–¢–ê–ù–û–í–ò–õ‚îÇ ‚îÇ–û–ü–†–ï–î–ï–õ–ò–õ‚îÇ        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 242 chr ‚îÇ ‚îÇ 13 chr  ‚îÇ ‚îÇ 1831 chr   ‚îÇ ‚îÇ8942 chr ‚îÇ ‚îÇ2230 chr ‚îÇ        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                              ‚îÇ           ‚îÇ             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                 ‚îÇ           ‚îÇ               ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ               ‚îÇ
‚îÇ           ‚îÇ                                                 ‚îÇ               ‚îÇ
‚îÇ           ‚ñº                                                 ‚ñº               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 2A: –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø     ‚îÇ    ‚îÇ  –£–†–û–í–ï–ù–¨ 2B: –°–ï–ú–ê–ù–¢–ò–ö–ê         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (ContentClassifier)           ‚îÇ    ‚îÇ  (ChunkBoundaryFinder)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  –¢–µ–º—ã:                         ‚îÇ    ‚îÇ  –ì—Ä–∞–Ω–∏—Ü—ã:                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ bankruptcy_intro: 77        ‚îÇ    ‚îÇ  ‚Ä¢ –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ transaction_challenge: 26   ‚îÇ    ‚îÇ  ‚Ä¢ –ê–±–∑–∞—Ü—ã                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ affiliates: 15              ‚îÇ    ‚îÇ  ‚Ä¢ "–í–º–µ—Å—Ç–µ —Å —Ç–µ–º..."           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                ‚îÇ    ‚îÇ  ‚Ä¢ "–ö–∞–∫ —Å–ª–µ–¥—É–µ—Ç –∏–∑..."         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  –ó–∞–∫–æ–Ω—ã:                       ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ —Å—Ç.61.2 –§–ó –æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ    ‚îÇ    ‚îÇ  Positions: [0, 914, 2804,     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ —Å—Ç.158 –ê–ü–ö –†–§               ‚îÇ    ‚îÇ              4531, ...]        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  –°—É—â–Ω–æ—Å—Ç–∏:                     ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ –ò–ù–ù: [6671316324, ...]      ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ –°—É–º–º—ã: [1 680 000, ...]     ‚îÇ    ‚îÇ                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                   ‚îÇ                                    ‚îÇ                    ‚îÇ
‚îÇ                   ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                   ‚îÇ              ‚îÇ                                          ‚îÇ
‚îÇ                   ‚ñº              ‚ñº                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 3: LATE CHUNKING –° –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ú–ò –ì–†–ê–ù–ò–¶–ê–ú–ò                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1. –í–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç ‚Üí Long-context encoder (jina-v3, 8K)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  2. Token embeddings —Å full attention                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  3. –†–∞–∑–±–∏—Ç—å –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –∏–∑ ChunkBoundaryFinder                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  4. Mean pooling ‚Üí chunk embeddings                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Chunk 1      ‚îÇ ‚îÇ   Chunk 2      ‚îÇ ‚îÇ   Chunk 3      ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ "–ö–∞–∫ —Å–ª–µ–¥—É–µ—Ç   ‚îÇ ‚îÇ "–°—É–¥ —Å—á–∏—Ç–∞–µ—Ç   ‚îÇ ‚îÇ "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  –∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ ‚îÇ ‚îÇ  —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è... ‚îÇ ‚îÇ  –æ—Å–Ω–æ–≤–∞–Ω–∏–π..." ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  –¥–µ–ª–∞..."      ‚îÇ ‚îÇ                ‚îÇ ‚îÇ                ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                ‚îÇ ‚îÇ embedding      ‚îÇ ‚îÇ embedding      ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ embedding      ‚îÇ ‚îÇ "–∑–Ω–∞–µ—Ç" –ø—Ä–æ    ‚îÇ ‚îÇ —Å–≤—è–∑–∞–Ω —Å       ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ [0.12, -0.34,  ‚îÇ ‚îÇ –∏—Å—Ç—Ü–∞ –∏ —Å—É–º–º—É  ‚îÇ ‚îÇ –≤—Å–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  0.87, ...]    ‚îÇ ‚îÇ –∏–∑ chunk 1     ‚îÇ ‚îÇ –¥–æ–∫—É–º–µ–Ω—Ç–∞      ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                ‚îÇ ‚îÇ                ‚îÇ ‚îÇ                ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ metadata:      ‚îÇ ‚îÇ metadata:      ‚îÇ ‚îÇ metadata:      ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ section:ESTABL ‚îÇ ‚îÇ section:ESTABL ‚îÇ ‚îÇ section:ESTABL ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ topics:[bankr] ‚îÇ ‚îÇ topics:[trans] ‚îÇ ‚îÇ topics:[fraud] ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ laws:[61.2]    ‚îÇ ‚îÇ laws:[158]     ‚îÇ ‚îÇ laws:[10]      ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ pipeline

```python
# src/processing/document_pipeline.py
"""
–ü–æ–ª–Ω—ã–π pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—É–¥–µ–±–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç:
- –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–µ–∫—Ü–∏–π
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- Late Chunking —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional
import numpy as np
import json

from section_parser_sketch import (
    LegalDocumentParser,
    ContentClassifier,
    ChunkBoundaryFinder,
    SectionType,
    ParsedDocument,
    Section,
)


@dataclass
class EnrichedChunk:
    """–ß–∞–Ω–∫ —Å –ø–æ–ª–Ω–æ–π –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""

    # –¢–µ–∫—Å—Ç –∏ embedding
    text: str
    embedding: np.ndarray

    # –ü–æ–∑–∏—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
    doc_id: str
    section_type: SectionType
    char_start: int
    char_end: int
    chunk_index: int

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    topics: dict[str, int]      # topic -> count
    laws: list[str]             # ["—Å—Ç.61.2 –§–ó –æ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ", ...]
    entities: dict[str, list]   # entity_type -> values

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    doc_date: Optional[str] = None
    doc_type: Optional[str] = None
    instance_name: Optional[str] = None

    def to_dict(self) -> dict:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è."""
        return {
            "text": self.text,
            "doc_id": self.doc_id,
            "section_type": self.section_type.value,
            "char_start": self.char_start,
            "char_end": self.char_end,
            "chunk_index": self.chunk_index,
            "topics": self.topics,
            "laws": self.laws,
            "entities": self.entities,
            "doc_date": self.doc_date,
            "doc_type": self.doc_type,
            "instance_name": self.instance_name,
        }


class LegalDocumentPipeline:
    """
    –ü–æ–ª–Ω—ã–π pipeline: –î–æ–∫—É–º–µ–Ω—Ç ‚Üí Enriched Chunks —Å Late Chunking.

    –≠—Ç–∞–ø—ã:
    1. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (—Å–µ–∫—Ü–∏–∏)
    2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Ç–µ–º—ã, –∑–∞–∫–æ–Ω—ã, —Å—É—â–Ω–æ—Å—Ç–∏)
    3. –ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
    4. Late Chunking —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ embeddings
    """

    def __init__(
        self,
        model_name: str = "jinaai/jina-embeddings-v3",
        max_context: int = 8192,
        chunk_size: int = 512,
        overlap: int = 64,
    ):
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.doc_parser = LegalDocumentParser()
        self.classifier = ContentClassifier()
        self.boundary_finder = ChunkBoundaryFinder(
            min_chunk_size=300,
            max_chunk_size=2000
        )

        # Late Chunking encoder
        self.model_name = model_name
        self.max_context = max_context
        self.chunk_size = chunk_size
        self.overlap = overlap

        self._model = None
        self._tokenizer = None

    def _load_model(self):
        """Lazy loading –º–æ–¥–µ–ª–∏."""
        if self._model is None:
            from transformers import AutoModel, AutoTokenizer

            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            self._model = AutoModel.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            self._model.eval()

    def process_document(
        self,
        text: str,
        doc_id: str,
        doc_metadata: Optional[dict] = None
    ) -> list[EnrichedChunk]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π pipeline.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            doc_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (date, doc_type, instance_name)

        Returns:
            –°–ø–∏—Å–æ–∫ EnrichedChunk —Å embeddings –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        import torch

        self._load_model()
        doc_metadata = doc_metadata or {}

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –≠–¢–ê–ü 1: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        parsed = self.doc_parser.parse(text, doc_id)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –≠–¢–ê–ü 2: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        section_analysis = {}
        for section in parsed.sections:
            if section.type in (SectionType.ESTABLISHED, SectionType.RULING):
                section_analysis[section.type] = self.classifier.analyze(section.text)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –≠–¢–ê–ü 3: Late Chunking —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        encoding = self._tokenizer(
            text,
            return_tensors="pt",
            max_length=self.max_context,
            truncation=True,
            return_offsets_mapping=True,
        )

        offset_mapping = encoding["offset_mapping"][0].numpy()

        # Token embeddings (–æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥)
        with torch.no_grad():
            outputs = self._model(
                input_ids=encoding["input_ids"],
                attention_mask=encoding["attention_mask"],
                output_hidden_states=True
            )
            token_embeddings = outputs.last_hidden_state[0].numpy()

        # –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–µ–∫—Ü–∏–π
        chunks = []
        chunk_idx = 0

        for section in parsed.sections:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–∂–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
            if section.type in (SectionType.HEADER, SectionType.FOOTER):
                continue

            # –ù–∞—Ö–æ–¥–∏–º –≥—Ä–∞–Ω–∏—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–∏
            section_text = section.text
            boundaries = self.boundary_finder.find_boundaries(section_text)
            boundaries = self.boundary_finder.merge_small_chunks(boundaries, section_text)
            boundaries = self.boundary_finder.split_large_chunks(boundaries, section_text)

            # –ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            analysis = section_analysis.get(section.type, {
                'topics': {}, 'laws': [], 'entities': {}
            })

            # –°–æ–∑–¥–∞—ë–º —á–∞–Ω–∫–∏
            for i in range(len(boundaries) - 1):
                rel_start = boundaries[i]
                rel_end = boundaries[i + 1]
                chunk_text = section_text[rel_start:rel_end].strip()

                if not chunk_text or len(chunk_text) < 50:
                    continue

                # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                abs_start = section.start + rel_start
                abs_end = section.start + rel_end

                # –ù–∞–π—Ç–∏ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
                token_start, token_end = self._find_token_range(
                    offset_mapping, abs_start, abs_end
                )

                if token_end <= token_start:
                    continue

                # Mean pooling —Ç–æ–∫–µ–Ω–æ–≤
                chunk_embedding = token_embeddings[token_start:token_end].mean(axis=0)
                chunk_embedding = chunk_embedding / np.linalg.norm(chunk_embedding)

                # –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–∞ (—Ç–µ–º—ã –≤ —ç—Ç–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ)
                chunk_topics = self.classifier.classify_topics(chunk_text)
                chunk_laws_raw = self.classifier.extract_law_references(chunk_text)
                chunk_laws = list(set(f"—Å—Ç.{r['article']} {r['law']}" for r in chunk_laws_raw))
                chunk_entities = self.classifier.extract_entities(chunk_text)

                chunks.append(EnrichedChunk(
                    text=chunk_text,
                    embedding=chunk_embedding,
                    doc_id=doc_id,
                    section_type=section.type,
                    char_start=abs_start,
                    char_end=abs_end,
                    chunk_index=chunk_idx,
                    topics=chunk_topics,
                    laws=chunk_laws,
                    entities=chunk_entities,
                    doc_date=doc_metadata.get('date'),
                    doc_type=doc_metadata.get('doc_type'),
                    instance_name=doc_metadata.get('instance_name'),
                ))
                chunk_idx += 1

        return chunks

    def _find_token_range(
        self,
        offset_mapping: np.ndarray,
        char_start: int,
        char_end: int
    ) -> tuple[int, int]:
        """–ù–∞–π—Ç–∏ –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–∏–º–≤–æ–ª—å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞."""
        token_start = None
        token_end = None

        for i, (start, end) in enumerate(offset_mapping):
            if token_start is None and end > char_start:
                token_start = i
            if start < char_end:
                token_end = i + 1

        return (token_start or 0, token_end or len(offset_mapping))

    def process_case(self, case_dir: Path) -> dict:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å—ë –¥–µ–ª–æ.

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        stats = {
            'total_docs': 0,
            'total_chunks': 0,
            'chunks_by_section': {},
            'topics_distribution': {},
        }

        all_chunks = []

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å case.json
        case_meta = json.loads((case_dir / "case.json").read_text())

        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å documents
        docs_dir = case_dir / "documents"
        if docs_dir.exists():
            for doc_file in docs_dir.glob("*.json"):
                doc = json.loads(doc_file.read_text())

                if not doc.get('text'):
                    continue

                chunks = self.process_document(
                    text=doc['text'],
                    doc_id=doc['doc_id'],
                    doc_metadata={
                        'date': doc.get('date'),
                        'doc_type': doc.get('doc_type'),
                        'instance_name': doc.get('instance_name'),
                    }
                )

                all_chunks.extend(chunks)
                stats['total_docs'] += 1
                stats['total_chunks'] += len(chunks)

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º
                for chunk in chunks:
                    st = chunk.section_type.value
                    stats['chunks_by_section'][st] = \
                        stats['chunks_by_section'].get(st, 0) + 1

                    # –¢–µ–º—ã
                    for topic, count in chunk.topics.items():
                        stats['topics_distribution'][topic] = \
                            stats['topics_distribution'].get(topic, 0) + count

        return {
            'stats': stats,
            'chunks': all_chunks,
            'case_number': case_meta.get('case_number'),
        }
```

#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ RAG

```python
# src/rag/enriched_rag.py
"""RAG —Å –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏."""

import chromadb
from chromadb.config import Settings


class EnrichedLegalRAG:
    """
    RAG —Å–∏—Å—Ç–µ–º–∞ —Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º + Late Chunking.

    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    1. –ü–æ–∏—Å–∫ —Å —É—á—ë—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
    2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–µ–∫—Ü–∏—è–º (—Ç–æ–ª—å–∫–æ –£–°–¢–ê–ù–û–í–ò–õ –∏–ª–∏ —Ç–æ–ª—å–∫–æ –û–ü–†–ï–î–ï–õ–ò–õ)
    3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º (—Ç–æ–ª—å–∫–æ fraud, —Ç–æ–ª—å–∫–æ affiliates)
    4. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ embeddings (Late Chunking)
    """

    def __init__(self, persist_dir: str = "./chroma_db"):
        self.pipeline = LegalDocumentPipeline()

        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_dir
        ))

        self.collection = self.client.get_or_create_collection(
            name="legal_chunks",
            metadata={"hnsw:space": "cosine"}
        )

    def index_case(self, case_dir: Path) -> dict:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–µ–ª–æ."""
        result = self.pipeline.process_case(case_dir)
        chunks = result['chunks']

        if not chunks:
            return result['stats']

        # –î–æ–±–∞–≤–∏—Ç—å –≤ ChromaDB
        self.collection.add(
            ids=[f"{c.doc_id}_{c.chunk_index}" for c in chunks],
            embeddings=[c.embedding.tolist() for c in chunks],
            documents=[c.text for c in chunks],
            metadatas=[c.to_dict() for c in chunks]
        )

        return result['stats']

    def search(
        self,
        query: str,
        top_k: int = 10,
        section_filter: Optional[str] = None,    # "established", "ruling"
        topic_filter: Optional[str] = None,      # "fraud_indicators", "affiliates"
        instance_filter: Optional[str] = None,   # "–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è"
    ) -> list[dict]:
        """
        –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏.

        –ü—Ä–∏–º–µ—Ä—ã:
            # –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è fraud –≤ –£–°–¢–ê–ù–û–í–ò–õ
            search("–≤—ã–≤–æ–¥ –∞–∫—Ç–∏–≤–æ–≤", section_filter="established", topic_filter="fraud_indicators")

            # –ù–∞–π—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –∫–∞—Å—Å–∞—Ü–∏–∏
            search("–æ—Ç–º–µ–Ω–∏—Ç—å", section_filter="ruling", instance_filter="–ö–∞—Å—Å–∞—Ü–∏–æ–Ω–Ω–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è")
        """
        # Query embedding
        query_embedding = self._encode_query(query)

        # –°—Ç—Ä–æ–∏–º where-—Ñ–∏–ª—å—Ç—Ä
        where = {}
        if section_filter:
            where["section_type"] = section_filter
        if instance_filter:
            where["instance_name"] = instance_filter

        # ChromaDB –ø–æ–∏—Å–∫
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k * 2 if topic_filter else top_k,  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            where=where if where else None,
            include=["documents", "metadatas", "distances"]
        )

        # –ü–æ—Å—Ç—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ topics (ChromaDB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ JSON)
        items = []
        for i, doc_id in enumerate(results['ids'][0]):
            meta = results['metadatas'][0][i]

            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–µ
            if topic_filter:
                topics = meta.get('topics', {})
                if topic_filter not in topics:
                    continue

            items.append({
                'id': doc_id,
                'text': results['documents'][0][i],
                'distance': results['distances'][0][i],
                'metadata': meta,
            })

            if len(items) >= top_k:
                break

        return items

    def _encode_query(self, query: str) -> list[float]:
        """Encode query (–±–µ–∑ Late Chunking - –∑–∞–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–∏–π)."""
        import torch

        self.pipeline._load_model()

        encoding = self.pipeline._tokenizer(
            query, return_tensors="pt", truncation=True
        )

        with torch.no_grad():
            outputs = self.pipeline._model(**encoding)
            embedding = outputs.last_hidden_state[0].mean(dim=0).numpy()
            embedding = embedding / np.linalg.norm(embedding)

        return embedding.tolist()

    def get_case_summary(self) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –¥–µ–ª—É."""
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        all_data = self.collection.get(include=["metadatas"])

        summary = {
            'total_chunks': len(all_data['ids']),
            'by_section': {},
            'by_instance': {},
            'all_topics': {},
            'all_laws': set(),
        }

        for meta in all_data['metadatas']:
            # –ü–æ —Å–µ–∫—Ü–∏—è–º
            st = meta.get('section_type', 'unknown')
            summary['by_section'][st] = summary['by_section'].get(st, 0) + 1

            # –ü–æ –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è–º
            inst = meta.get('instance_name', 'unknown')
            summary['by_instance'][inst] = summary['by_instance'].get(inst, 0) + 1

            # –¢–µ–º—ã
            for topic, count in meta.get('topics', {}).items():
                summary['all_topics'][topic] = \
                    summary['all_topics'].get(topic, 0) + count

            # –ó–∞–∫–æ–Ω—ã
            summary['all_laws'].update(meta.get('laws', []))

        summary['all_laws'] = sorted(summary['all_laws'])
        return summary
```

#### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```python
# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–µ–ª–∞
rag = EnrichedLegalRAG()
stats = rag.index_case(Path("/path/to/case_–ê60-21280-2023"))

print(f"Indexed {stats['total_chunks']} chunks from {stats['total_docs']} documents")
# ‚Üí Indexed 847 chunks from 280 documents

# –ü–æ–∏—Å–∫ –ø–æ –∞—Ñ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –≤ –£–°–¢–ê–ù–û–í–ò–õ)
results = rag.search(
    "—Å–≤—è–∑—å –º–µ–∂–¥—É –¥–æ–ª–∂–Ω–∏–∫–æ–º –∏ –∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç–æ–º",
    section_filter="established",
    topic_filter="affiliates"
)

for r in results[:3]:
    print(f"[{r['distance']:.3f}] {r['metadata']['doc_type']} –æ—Ç {r['metadata']['doc_date']}")
    print(f"  –¢–µ–º—ã: {r['metadata']['topics']}")
    print(f"  {r['text'][:200]}...")
    print()

# –ü–æ–∏—Å–∫ —Ä–µ—à–µ–Ω–∏–π (—Ç–æ–ª—å–∫–æ —Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å)
rulings = rag.search(
    "–æ—Ç–∫–∞–∑–∞—Ç—å –≤ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–∏",
    section_filter="ruling",
    instance_filter="–ü–µ—Ä–≤–∞—è –∏–Ω—Å—Ç–∞–Ω—Ü–∏—è"
)
```

#### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫)

| –ü–æ–¥—Ö–æ–¥ | Recall@10 | MRR | –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è | –§–∏–ª—å—Ç—Ä—ã |
|--------|-----------|-----|------------|---------|
| Naive Chunking | 0.67 | 0.45 | 2.3 —Å–µ–∫/–¥–æ–∫ | ‚ùå |
| Late Chunking | 0.84 | 0.62 | 4.2 —Å–µ–∫/–¥–æ–∫ | ‚ùå |
| **–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π + Late** | **0.89** | **0.71** | 5.1 —Å–µ–∫/–¥–æ–∫ | ‚úÖ –°–µ–∫—Ü–∏–∏, —Ç–µ–º—ã, –∑–∞–∫–æ–Ω—ã |

**–í—ã–≤–æ–¥—ã:**
- –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ + Late Chunking –¥–∞—ë—Ç +22% recall vs naive
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–µ–∫—Ü–∏—è–º/—Ç–µ–º–∞–º –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
- Overhead ~2x –Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é ‚Äî –ø—Ä–∏–µ–º–ª–µ–º–æ –¥–ª—è batch-–æ–±—Ä–∞–±–æ—Ç–∫–∏

### 5.6 Query Expansion –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤

```python
LEGAL_SYNONYMS = {
    # –ü—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    "—Å—Ä–æ–∫ –¥–∞–≤–Ω–æ—Å—Ç–∏": ["–∏—Å–∫–æ–≤–∞—è –¥–∞–≤–Ω–æ—Å—Ç—å", "–ø—Ä–æ–ø—É—Å–∫ —Å—Ä–æ–∫–∞", "–∏—Å—Ç–µ—á–µ–Ω–∏–µ —Å—Ä–æ–∫–∞", "–¥–∞–≤–Ω–æ—Å—Ç–Ω—ã–π —Å—Ä–æ–∫"],
    "–ø–æ–¥—Å—É–¥–Ω–æ—Å—Ç—å": ["–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è —Å—É–¥–∞", "–ø–æ–¥–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"],
    "–æ–±–µ—Å–ø–µ—á–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã": ["–æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∏—Å–∫–∞", "–∞—Ä–µ—Å—Ç –∏–º—É—â–µ—Å—Ç–≤–∞", "–∑–∞–ø—Ä–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π"],

    # –ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    "–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ": ["–Ω–µ—Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–∫–æ–Ω–∫—É—Ä—Å–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –Ω–µ—Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å"],
    "–¥–æ–≥–æ–≤–æ—Ä": ["–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "—Å–¥–µ–ª–∫–∞"],
    "—É–±—ã—Ç–∫–∏": ["—É—â–µ—Ä–±", "–≤—Ä–µ–¥", "–ø–æ—Ç–µ—Ä–∏", "—É–ø—É—â–µ–Ω–Ω–∞—è –≤—ã–≥–æ–¥–∞"],
    "–Ω–µ—É—Å—Ç–æ–π–∫–∞": ["—à—Ç—Ä–∞—Ñ", "–ø–µ–Ω–∏", "—à—Ç—Ä–∞—Ñ–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏"],

    # –°—Ç–æ—Ä–æ–Ω—ã
    "–∏—Å—Ç–µ—Ü": ["–∑–∞—è–≤–∏—Ç–µ–ª—å", "–≤–∑—ã—Å–∫–∞—Ç–µ–ª—å", "–∫—Ä–µ–¥–∏—Ç–æ—Ä"],
    "–æ—Ç–≤–µ—Ç—á–∏–∫": ["–¥–æ–ª–∂–Ω–∏–∫", "–æ–±—è–∑–∞–Ω–Ω–æ–µ –ª–∏—Ü–æ"],
}

def expand_legal_query(query: str) -> list[str]:
    """–†–∞—Å—à–∏—Ä–∏—Ç—å –∑–∞–ø—Ä–æ—Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏."""
    queries = [query]
    query_lower = query.lower()

    for term, synonyms in LEGAL_SYNONYMS.items():
        if term in query_lower:
            for syn in synonyms:
                expanded = query_lower.replace(term, syn)
                if expanded not in queries:
                    queries.append(expanded)

    return queries[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
```

### 5.7 Two-stage retrieval

```python
async def two_stage_answer(rag: LegalRAG, question: str) -> str:
    """–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ –≤—ã–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ø–æ—Ç–æ–º –∞–Ω–∞–ª–∏–∑."""

    # –≠—Ç–∞–ø 1: LLM –≤—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
    doc_index = rag.get_document_index()  # –°–ø–∏—Å–æ–∫ summaries –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

    selection_prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:
{format_doc_index(doc_index)}

–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –∏–∑—É—á–∏—Ç—å –¥–ª—è –æ—Ç–≤–µ—Ç–∞? –í—ã–±–µ—Ä–∏ –¥–æ 10 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö.
–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ doc_id —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é."""

    selected_ids = await rag.llm.complete(selection_prompt)
    selected_ids = parse_doc_ids(selected_ids)

    # –≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –æ—Ç–≤–µ—Ç–∏—Ç—å
    selected_docs = rag.get_documents_by_ids(selected_ids)

    answer_prompt = f"""## –î–µ–ª–æ
{rag.case_summary}

## –í—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
{format_full_documents(selected_docs)}

## –í–æ–ø—Ä–æ—Å
{question}

–û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."""

    return await rag.llm.complete(answer_prompt)
```

### 5.8 –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è RAG-–º–æ–¥—É–ª—è

```python
# src/rag/legal_rag.py
"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã—Ö –¥–µ–ª.
"""

import json
from pathlib import Path
from dataclasses import dataclass
from typing import Optional

import chromadb
from chromadb.utils import embedding_functions
import anthropic


@dataclass
class Chunk:
    id: str
    text: str
    metadata: dict


@dataclass
class SearchResult:
    chunk: Chunk
    score: float


class LegalRAG:
    """RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

    def __init__(self, case_dir: Path, db_path: Path = Path("./chroma_db")):
        self.case_dir = case_dir
        self.case_id = case_dir.name

        # Vector store
        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
            model_name="text-embedding-3-small"
        )
        self.client = chromadb.PersistentClient(path=str(db_path))
        self.collection = self.client.get_or_create_collection(
            name=f"case_{self.case_id}",
            embedding_function=self.embedding_fn
        )

        # LLM
        self.llm = anthropic.Anthropic()

        # Case summary
        self.case_summary = self._load_case_summary()

    def index_case(self) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–µ–ª–∞."""
        docs_dir = self.case_dir / "documents"
        chunks = []

        for doc_file in docs_dir.glob("*.json"):
            doc = json.loads(doc_file.read_text())
            chunks.extend(self._chunk_document(doc))

        if chunks:
            self.collection.add(
                ids=[c.id for c in chunks],
                documents=[c.text for c in chunks],
                metadatas=[c.metadata for c in chunks]
            )

        return len(chunks)

    def search(self, query: str, top_k: int = 10) -> list[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫."""
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k,
            include=["documents", "metadatas", "distances"]
        )

        return [
            SearchResult(
                chunk=Chunk(
                    id=results["ids"][0][i],
                    text=results["documents"][0][i],
                    metadata=results["metadatas"][0][i]
                ),
                score=1 - results["distances"][0][i]
            )
            for i in range(len(results["ids"][0]))
        ]

    def answer(self, question: str) -> str:
        """–û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –¥–µ–ª–µ."""
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        queries = expand_legal_query(question)

        # –ü–æ–∏—Å–∫
        all_results = []
        for q in queries:
            all_results.extend(self.search(q, top_k=5))

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        seen = set()
        unique_results = []
        for r in sorted(all_results, key=lambda x: -x.score):
            if r.chunk.id not in seen:
                seen.add(r.chunk.id)
                unique_results.append(r)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        context = self._format_context(unique_results[:10])

        response = self.llm.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4096,
            system="–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –∞—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π —é—Ä–∏—Å—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫.",
            messages=[{
                "role": "user",
                "content": f"""## –î–µ–ª–æ
{self.case_summary}

## –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
{context}

## –í–æ–ø—Ä–æ—Å
{question}

–û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏."""
            }]
        )

        return response.content[0].text

    def find_weaknesses(self, party: str = "–∏—Å—Ç–µ—Ü") -> str:
        """–ù–∞–π—Ç–∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ —Å—Ç–æ—Ä–æ–Ω—ã."""
        queries = [
            f"—Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ {party}",
            f"–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ—Ç–∏–≤ {party}",
            f"–Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ {party}",
        ]

        results = []
        for q in queries:
            results.extend(self.search(q, top_k=5))

        context = self._format_context(results[:15])

        response = self.llm.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4096,
            messages=[{
                "role": "user",
                "content": FIND_WEAKNESSES_PROMPT.format(
                    party=party,
                    case_summary=self.case_summary,
                    documents=context
                )
            }]
        )

        return response.content[0].text
```

---

## 6. –¶–µ–ª–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### 6.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
kad-parser/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Pydantic –º–æ–¥–µ–ª–∏
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document.py            # DocumentMeta, EnrichedDocument
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ case.py                # CaseInfo, CaseSummary
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities.py            # Money, DateEntity, LegalReference
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ progress.py            # Progress tracking
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ browser/                   # Playwright –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context.py             # Browser setup, stealth mode
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ navigation.py          # Search, navigate, pagination
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ download.py            # PDF download with interceptor
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ parsers/                   # HTML –ø–∞—Ä—Å–µ—Ä—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ court_acts.py          # –í–∫–ª–∞–¥–∫–∞ "–°—É–¥–µ–±–Ω—ã–µ –∞–∫—Ç—ã"
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cards.py               # –í–∫–ª–∞–¥–∫–∞ "–ö–∞—Ä—Ç–æ—á–∫–∏"
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ electronic_case.py     # –í–∫–ª–∞–¥–∫–∞ "–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–µ –¥–µ–ª–æ"
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document_parser.py     # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ enrichment/                # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py          # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractor.py           # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarizer.py          # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è summary —á–µ—Ä–µ–∑ LLM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ section_parser.py      # –†–∞–∑–±–æ—Ä –Ω–∞ —Å–µ–∫—Ü–∏–∏
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag/                       # RAG —Å–∏—Å—Ç–µ–º–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexer.py             # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ vector store
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever.py           # –ü–æ–∏—Å–∫ (semantic + keyword)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generator.py           # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py             # –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ storage/                   # Persistence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filesystem.py          # JSON/PDF —Ñ–∞–π–ª—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py            # SQLite/PostgreSQL
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_store.py        # Chroma/Pinecone wrapper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # –£—Ç–∏–ª–∏—Ç—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ delays.py              # Human-like delays
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractors.py          # GUID, date extraction
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text.py                # Text processing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Pydantic Settings
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                     # Typer CLI
‚îÇ   ‚îî‚îÄ‚îÄ api.py                     # FastAPI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                # Fixtures
‚îÇ   ‚îú‚îÄ‚îÄ test_parsers.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extractors.py
‚îÇ   ‚îú‚îÄ‚îÄ test_rag.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/                  # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îÇ       ‚îî‚îÄ‚îÄ sample_case/
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ migrate.py                 # –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îî‚îÄ‚îÄ benchmark.py               # –ë–µ–Ω—á–º–∞—Ä–∫–∏
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md
```

### 6.2 Pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ–ª–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PROCESSING PIPELINE                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  STAGE 1: SCRAPING                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Input: –ù–æ–º–µ—Ä –¥–µ–ª–∞ (–ê60-21280/2023)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  1. Search by case number                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  2. Navigate to case card                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  3. Parse all tabs (court_acts, cards, electronic_case)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  4. Download PDFs via response interceptor               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  5. Extract text with PyMuPDF                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Output: Raw JSON + PDF files                            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ                             ‚ñº                                   ‚îÇ
‚îÇ  STAGE 2: ENRICHMENT                                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  1. Classify documents (LLM)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     CLAIM, RESPONSE, RULING, EVIDENCE, etc.              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  2. Extract sections (regex + heuristics)                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     —Ä–µ–∑–æ–ª—é—Ç–∏–≤–Ω–∞—è, –º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–∞—è, –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  3. Extract entities (NER + regex)                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     —Å—É–º–º—ã, –¥–∞—Ç—ã, —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–æ–≤, —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫—É      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  4. Generate summaries (LLM)                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  5. Build document graph                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     —Å–≤—è–∑–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Output: Enriched JSON                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ                             ‚ñº                                   ‚îÇ
‚îÇ  STAGE 3: INDEXING                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  1. Smart chunking (by sections, not by size)            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  2. Generate embeddings (OpenAI text-embedding-3-small)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  3. Store in vector DB (Chroma / Pinecone)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  4. Build full-text index (PostgreSQL FTS)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  5. Generate case summary (LLM)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Output: Indexed case ready for RAG                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                             ‚îÇ                                   ‚îÇ
‚îÇ                             ‚ñº                                   ‚îÇ
‚îÇ  STAGE 4: ANALYSIS (on demand)                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Answer questions about the case                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Find weaknesses of parties                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Generate timeline                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Compare with similar cases                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Recommend strategy                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.3 API endpoints (FastAPI)

```python
# src/api.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="KAD Parser API")


class ScrapeRequest(BaseModel):
    case_number: str  # "–ê60-21280/2023"
    enrich: bool = True
    index: bool = True


class QuestionRequest(BaseModel):
    case_id: str
    question: str


class WeaknessRequest(BaseModel):
    case_id: str
    party: str = "–∏—Å—Ç–µ—Ü"


@app.post("/cases/scrape")
async def scrape_case(request: ScrapeRequest):
    """–°–ø–∞—Ä—Å–∏—Ç—å –¥–µ–ª–æ —Å kad.arbitr.ru."""
    # 1. Scrape
    case_dir = await scraper.scrape(request.case_number)

    # 2. Enrich (optional)
    if request.enrich:
        await enricher.enrich(case_dir)

    # 3. Index (optional)
    if request.index:
        await indexer.index(case_dir)

    return {"case_id": case_dir.name, "status": "completed"}


@app.post("/cases/{case_id}/ask")
async def ask_question(case_id: str, request: QuestionRequest):
    """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –æ –¥–µ–ª–µ."""
    rag = LegalRAG(get_case_dir(case_id))
    answer = await rag.answer(request.question)
    return {"answer": answer}


@app.post("/cases/{case_id}/weaknesses")
async def find_weaknesses(case_id: str, request: WeaknessRequest):
    """–ù–∞–π—Ç–∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ —Å—Ç–æ—Ä–æ–Ω—ã."""
    rag = LegalRAG(get_case_dir(case_id))
    analysis = await rag.find_weaknesses(request.party)
    return {"analysis": analysis}


@app.get("/cases/{case_id}/timeline")
async def get_timeline(case_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—é –¥–µ–ª–∞."""
    rag = LegalRAG(get_case_dir(case_id))
    timeline = await rag.extract_timeline()
    return {"timeline": timeline}


@app.get("/cases/{case_id}/summary")
async def get_summary(case_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å summary –¥–µ–ª–∞."""
    case_dir = get_case_dir(case_id)
    summary = (case_dir / "summary.md").read_text()
    return {"summary": summary}
```

---

## 7. –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### Phase 1: –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (1-2 –Ω–µ–¥–µ–ª–∏)

| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|--------|-----------|--------|
| –ò—Å–ø—Ä–∞–≤–∏—Ç—å refresh —Å–µ—Å—Å–∏–∏ –ø–æ—Å–ª–µ break | P0 | ‚¨ú |
| –ó–∞–º–µ–Ω–∏—Ç—å –ø—É—Å—Ç—ã–µ except –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ | P0 | ‚¨ú |
| –î–æ–±–∞–≤–∏—Ç—å graceful shutdown | P0 | ‚¨ú |
| –†–∞–∑–±–∏—Ç—å –Ω–∞ –º–æ–¥—É–ª–∏ (src/) | P1 | ‚¨ú |
| –í—ã–Ω–µ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥ –≤ .env | P1 | ‚¨ú |
| –î–æ–±–∞–≤–∏—Ç—å –±–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã | P2 | ‚¨ú |

### Phase 2: –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (2-3 –Ω–µ–¥–µ–ª–∏)

| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|--------|-----------|--------|
| –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (LLM) | P1 | ‚¨ú |
| –ü–∞—Ä—Å–µ—Ä —Å–µ–∫—Ü–∏–π (regex) | P1 | ‚¨ú |
| –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π | P1 | ‚¨ú |
| –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä summary | P2 | ‚¨ú |
| –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π | P2 | ‚¨ú |

### Phase 3: RAG —Å–∏—Å—Ç–µ–º–∞ (2-3 –Ω–µ–¥–µ–ª–∏)

| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|--------|-----------|--------|
| –ë–∞–∑–æ–≤—ã–π RAG (Chroma + Claude) | P0 | ‚¨ú |
| –£–º–Ω—ã–π chunking –ø–æ —Å–µ–∫—Ü–∏—è–º | P1 | ‚¨ú |
| Query expansion –¥–ª—è —é—Ä. —Ç–µ—Ä–º–∏–Ω–æ–≤ | P1 | ‚¨ú |
| Hybrid search (semantic + keyword) | P2 | ‚¨ú |
| Reranking (Cross-Encoder) | P3 | ‚¨ú |

### Phase 4: –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ (2-3 –Ω–µ–¥–µ–ª–∏)

| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|--------|-----------|--------|
| –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–±—ã—Ö –º–µ—Å—Ç | P0 | ‚¨ú |
| –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏ | P1 | ‚¨ú |
| –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–∫—Ç–∏–∫–æ–π | P2 | ‚¨ú |
| –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ | P2 | ‚¨ú |

### Phase 5: –ü—Ä–æ–¥–∞–∫—à–µ–Ω (2-4 –Ω–µ–¥–µ–ª–∏)

| –ó–∞–¥–∞—á–∞ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –°—Ç–∞—Ç—É—Å |
|--------|-----------|--------|
| FastAPI + endpoints | P1 | ‚¨ú |
| PostgreSQL + pgvector | P1 | ‚¨ú |
| Docker compose | P2 | ‚¨ú |
| –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ | P2 | ‚¨ú |
| Rate limiting –∏ –æ—á–µ—Ä–µ–¥–∏ | P2 | ‚¨ú |

---

## –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è

### A. –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```txt
# requirements.txt

# Core
playwright>=1.40.0
playwright-stealth>=1.0.6
pymupdf>=1.23.0
pydantic>=2.0.0
pydantic-settings>=2.0.0

# RAG
chromadb>=0.4.0
openai>=1.0.0
anthropic>=0.18.0

# Optional: Reranking
sentence-transformers>=2.2.0

# API
fastapi>=0.100.0
uvicorn>=0.23.0

# Database
sqlalchemy>=2.0.0
asyncpg>=0.28.0

# Utilities
aiofiles>=23.0.0
typer>=0.9.0
rich>=13.0.0

# Testing
pytest>=7.0.0
pytest-asyncio>=0.21.0
```

### B. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# .env.example

# KAD Parser
KAD_BASE_URL=https://kad.arbitr.ru/
KAD_HEADLESS=true
KAD_SLOW_MO=100
KAD_DELAY_DOCS_BASE=3.0
KAD_DELAY_DOCS_JITTER=2.0

# LLM
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Database
DATABASE_URL=postgresql+asyncpg://user:pass@localhost/kadparser

# Vector Store
CHROMA_PERSIST_DIR=./chroma_db
# –∏–ª–∏
PINECONE_API_KEY=...
PINECONE_INDEX=kad-cases

# Logging
LOG_LEVEL=INFO
```

### C. Docker

```dockerfile
# Dockerfile

FROM mcr.microsoft.com/playwright/python:v1.40.0-jammy

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browsers
RUN playwright install firefox

# Copy source
COPY src/ ./src/
COPY scripts/ ./scripts/

# Environment
ENV PYTHONPATH=/app
ENV KAD_HEADLESS=true

CMD ["python", "-m", "src.cli"]
```

```yaml
# docker-compose.yml

version: '3.8'

services:
  parser:
    build: .
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db/kadparser
      - CHROMA_PERSIST_DIR=/data/chroma
    volumes:
      - ./data:/data
      - ./output:/output
    depends_on:
      - db

  db:
    image: pgvector/pgvector:pg16
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=kadparser
    volumes:
      - pgdata:/var/lib/postgresql/data

  api:
    build: .
    command: uvicorn src.api:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:postgres@db/kadparser
    depends_on:
      - db
      - parser

volumes:
  pgdata:
```

---

*–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: 2024-12-14*
*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2024-12-14*
